{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from shutil import copyfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: split_data\n",
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "  \"\"\"\n",
    "  Splits the data into train and test sets\n",
    "  \n",
    "  Args:\n",
    "    SOURCE_DIR (string): directory path containing the images\n",
    "    TRAINING_DIR (string): directory path to be used for training\n",
    "    VALIDATION_DIR (string): directory path to be used for validation\n",
    "    SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
    "    \n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  ### START CODE HERE\n",
    "  file_list = []\n",
    "  for i in os.listdir(SOURCE_DIR):\n",
    "    if os.path.getsize(os.path.join(SOURCE_DIR, i)) == 0:\n",
    "      print(i, 'is zero length, so ignoring.')\n",
    "      continue\n",
    "\n",
    "    file_list.append(i)\n",
    "\n",
    "  random_sample = random.sample(file_list, len(file_list))\n",
    "  indices = int(split_size*len(random_sample))\n",
    "  for i in random_sample[:indices]:\n",
    "    copyfile(os.path.join(SOURCE_DIR, i), os.path.join(TRAINING_DIR, i))\n",
    "  for i in random_sample[indices:]:\n",
    "    copyfile(os.path.join(SOURCE_DIR, i), os.path.join(VALIDATION_DIR, i))\n",
    "  ### END CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define root directory\n",
    "root_dir = './malaria/'\n",
    "\n",
    "# Empty directory to prevent FileExistsError is the function is run several times\n",
    "if os.path.exists(root_dir):\n",
    "  shutil.rmtree(root_dir)\n",
    "\n",
    "# GRADED FUNCTION: create_train_val_dirs\n",
    "def create_train_val_dirs(root_path):\n",
    "  \"\"\"\n",
    "  Creates directories for the train and test sets\n",
    "  \n",
    "  Args:\n",
    "    root_path (string) - the base directory path to create subdirectories from\n",
    "  \n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"  \n",
    "  ### START CODE HERE\n",
    "\n",
    "  # HINT:\n",
    "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
    "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
    "  os.makedirs(os.path.join(root_path, \"training/Parasitized\"))\n",
    "  os.makedirs(os.path.join(root_path, \"training/Uninfected\"))\n",
    "  os.makedirs(os.path.join(root_path, \"validation/Uninfected\"))\n",
    "  os.makedirs(os.path.join(root_path, \"validation/Parasitized\"))\n",
    "\n",
    "  ### END CODE HERE\n",
    "\n",
    "  \n",
    "try:\n",
    "  create_train_val_dirs(root_path=root_dir)\n",
    "except FileExistsError:\n",
    "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasitised = \"./cell_images/Parasitized/\"\n",
    "normal = \"./cell_images/Uninfected/\"\n",
    "\n",
    "training_dir = \"./malaria/training/\"\n",
    "validation_dir = \"./malaria/validation\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: split_data\n",
    "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
    "  \"\"\"\n",
    "  Splits the data into train and test sets\n",
    "  \n",
    "  Args:\n",
    "    SOURCE_DIR (string): directory path containing the images\n",
    "    TRAINING_DIR (string): directory path to be used for training\n",
    "    VALIDATION_DIR (string): directory path to be used for validation\n",
    "    SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
    "    \n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "\n",
    "  ### START CODE HERE\n",
    "  file_list = []\n",
    "  for i in os.listdir(SOURCE_DIR):\n",
    "    if os.path.getsize(os.path.join(SOURCE_DIR, i)) == 0:\n",
    "      print(i, 'is zero length, so ignoring.')\n",
    "      continue\n",
    "\n",
    "    file_list.append(i)\n",
    "\n",
    "  random_sample = random.sample(file_list, len(file_list))\n",
    "  indices = int(split_size*len(random_sample))\n",
    "  for i in random_sample[:indices]:\n",
    "    copyfile(os.path.join(SOURCE_DIR, i), os.path.join(TRAINING_DIR, i))\n",
    "  for i in random_sample[indices:]:\n",
    "    copyfile(os.path.join(SOURCE_DIR, i), os.path.join(VALIDATION_DIR, i))\n",
    "  ### END CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir_parasitised = os.path.join(training_dir, \"Parasitized\")\n",
    "training_dir_normal = os.path.join(training_dir, \"Uninfected\")\n",
    "validation_dir_parasitised = os.path.join(validation_dir, \"Parasitized\")\n",
    "validation_dir_normal = os.path.join(validation_dir, \"Uninfected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty directories in case you run this cell multiple times\n",
    "if len(os.listdir(training_dir_normal)) > 0:\n",
    "  for file in os.scandir(training_dir_normal):\n",
    "    os.remove(file.path)\n",
    "if len(os.listdir(training_dir_parasitised)) > 0:\n",
    "  for file in os.scandir(training_dir_parasitised):\n",
    "    os.remove(file.path)\n",
    "if len(os.listdir(validation_dir_normal)) > 0:\n",
    "  for file in os.scandir(validation_dir_normal):\n",
    "    os.remove(file.path)\n",
    "if len(os.listdir(validation_dir_parasitised)) > 0:\n",
    "  for file in os.scandir(validation_dir_parasitised):\n",
    "    os.remove(file.path)\n",
    "\n",
    "# Define proportion of images used for training\n",
    "split_size = .9\n",
    "\n",
    "# Run the function\n",
    "# NOTE: Messages about zero length images should be printed out\n",
    "split_data(normal, training_dir_normal, validation_dir_normal, split_size)\n",
    "split_data(parasitised, training_dir_parasitised, validation_dir_parasitised, split_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_val_generators\n",
    "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
    "  \"\"\"\n",
    "  Creates the training and validation data generators\n",
    "  \n",
    "  Args:\n",
    "    TRAINING_DIR (string): directory path containing the training images\n",
    "    VALIDATION_DIR (string): directory path containing the testing/validation images\n",
    "    \n",
    "  Returns:\n",
    "    train_generator, validation_generator - tuple containing the generators\n",
    "  \"\"\"\n",
    "  ### START CODE HERE\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  train_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "  # Pass in the appropiate arguments to the flow_from_directory method\n",
    "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
    "                                                      batch_size=1024,\n",
    "                                                      class_mode='binary',\n",
    "                                                      target_size=(128, 128))\n",
    "\n",
    "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
    "  validation_datagen = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "  # Pass in the appropiate arguments to the flow_from_directory method\n",
    "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
    "                                                                batch_size=512,\n",
    "                                                                class_mode='binary',\n",
    "                                                                target_size=(128, 128))\n",
    "  ### END CODE HERE\n",
    "  return train_generator, validation_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24802 images belonging to 2 classes.\n",
      "Found 2756 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator = train_val_generators(training_dir, validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 23s 642ms/step - loss: 1.2707 - accuracy: 0.5046 - val_loss: 0.6921 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 15s 614ms/step - loss: 0.6865 - accuracy: 0.5479 - val_loss: 0.6675 - val_accuracy: 0.6114\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 15s 612ms/step - loss: 0.6766 - accuracy: 0.5738 - val_loss: 0.6816 - val_accuracy: 0.6277\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 16s 627ms/step - loss: 0.6526 - accuracy: 0.6562 - val_loss: 0.6021 - val_accuracy: 0.7148\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 16s 630ms/step - loss: 0.5615 - accuracy: 0.7184 - val_loss: 0.5288 - val_accuracy: 0.7253\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 15s 608ms/step - loss: 0.4603 - accuracy: 0.7823 - val_loss: 0.4152 - val_accuracy: 0.8022\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 15s 610ms/step - loss: 0.4335 - accuracy: 0.7987 - val_loss: 0.3488 - val_accuracy: 0.8451\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 15s 605ms/step - loss: 0.3045 - accuracy: 0.8728 - val_loss: 0.2372 - val_accuracy: 0.9067\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 15s 608ms/step - loss: 0.2122 - accuracy: 0.9252 - val_loss: 0.1838 - val_accuracy: 0.9380\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 16s 650ms/step - loss: 0.1796 - accuracy: 0.9391 - val_loss: 0.1672 - val_accuracy: 0.9434\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 15s 606ms/step - loss: 0.1653 - accuracy: 0.9434 - val_loss: 0.1581 - val_accuracy: 0.9478\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 16s 654ms/step - loss: 0.1608 - accuracy: 0.9449 - val_loss: 0.1631 - val_accuracy: 0.9434\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 15s 611ms/step - loss: 0.1576 - accuracy: 0.9468 - val_loss: 0.1589 - val_accuracy: 0.9448\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 16s 624ms/step - loss: 0.1636 - accuracy: 0.9428 - val_loss: 0.1661 - val_accuracy: 0.9452\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 16s 621ms/step - loss: 0.1508 - accuracy: 0.9493 - val_loss: 0.1527 - val_accuracy: 0.9496\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 16s 617ms/step - loss: 0.1450 - accuracy: 0.9508 - val_loss: 0.1506 - val_accuracy: 0.9492\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 16s 627ms/step - loss: 0.1418 - accuracy: 0.9517 - val_loss: 0.1482 - val_accuracy: 0.9474\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 15s 610ms/step - loss: 0.1390 - accuracy: 0.9529 - val_loss: 0.1576 - val_accuracy: 0.9459\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 16s 617ms/step - loss: 0.1371 - accuracy: 0.9532 - val_loss: 0.1544 - val_accuracy: 0.9492\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 15s 616ms/step - loss: 0.1343 - accuracy: 0.9535 - val_loss: 0.1462 - val_accuracy: 0.9488\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 15s 609ms/step - loss: 0.1336 - accuracy: 0.9544 - val_loss: 0.1522 - val_accuracy: 0.9488\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 15s 610ms/step - loss: 0.1318 - accuracy: 0.9542 - val_loss: 0.1426 - val_accuracy: 0.9492\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 15s 609ms/step - loss: 0.1290 - accuracy: 0.9557 - val_loss: 0.1474 - val_accuracy: 0.9474\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 15s 609ms/step - loss: 0.1281 - accuracy: 0.9554 - val_loss: 0.1460 - val_accuracy: 0.9510\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 15s 609ms/step - loss: 0.1247 - accuracy: 0.9577 - val_loss: 0.1418 - val_accuracy: 0.9496\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: create_model\n",
    "def create_model():\n",
    "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
    "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
    "\n",
    "  ### START CODE HERE\n",
    "\n",
    "  model = tf.keras.models.Sequential([ \n",
    "      tf.keras.layers.Conv2D(16, (3, 3), activation = 'relu', padding = 'same', input_shape = (128,128,3)),\n",
    "      tf.keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "      tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', padding = 'same'),\n",
    "      tf.keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), activation = 'relu', padding = 'same'),\n",
    "      tf.keras.layers.MaxPool2D((3,3)),\n",
    "\n",
    "      tf.keras.layers.Flatten(),\n",
    "\n",
    "      tf.keras.layers.Dense(256, activation = 'relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "  ])\n",
    "\n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy']) \n",
    "    \n",
    "  ### END CODE HERE\n",
    "\n",
    "  return model\n",
    "\n",
    "# Get the untrained model\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "# Note that this may take some time.\n",
    "with tf.device(\"GPU:0\"):\n",
    "    history = model.fit(train_generator,\n",
    "                        epochs=25,\n",
    "                        verbose=1,\n",
    "                        validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: great_model_95pct\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: great_model_95pct\\assets\n"
     ]
    }
   ],
   "source": [
    "#model.save(\"great_model_95pct\")\n",
    "#model.save_weights(\"95_pct_weights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('tf2.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3f9b9e2c84dd319003698fc684ead22f769b71014b59713d46cc8251cf0f9de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
